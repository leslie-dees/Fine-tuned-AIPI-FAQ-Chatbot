{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = '1826dfeb-ad24-4d54-95c5-8b014de6eba1'\n",
    "\n",
    "# Connect to Pinecone for data access\n",
    "pc = Pinecone(api_key = os.environ['PINECONE_API_KEY'])\n",
    "index_name = 'aipi-chatbot'\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load in the embeddings model\n",
    "embeddings_model = SentenceTransformer(\"WhereIsAI/UAE-Large-V1\")\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# Free CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ADAPTER_ID = \"kahliahogg/mistral-bot\"\n",
    "\n",
    "# GPU/CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running evaluation on {device}\")\n",
    "\n",
    "# Load model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    ADAPTER_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_ID)\n",
    "\n",
    "# Load merged model into pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "Question = \"How many electives do I have to take?\"\n",
    "\n",
    "embeddings = embeddings_model.encode(Question)\n",
    "\n",
    "matches = index.query(\n",
    "    vector=embeddings.tolist(),\n",
    "    top_k = 2,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "DOCUMENTS = matches['matches'][0]['metadata']['text'] + matches['matches'][1]['metadata']['text']\n",
    "\n",
    "FEW_SHOT_PROMPT = {\n",
    "    'Q1': \"What is the Technical Core of the program? \\n Documents: AIPI 510, AIPI 520, AIPI 540, AIPI 561, and AIPI 501\",\n",
    "    'A1': \"Yes, the documents are relevant enough to the question\",\n",
    "    'Q2': \"Am I able to complete the curriculum without having to do an internship? \\n Documents: Internships apply engineering principles to solve one or more problems, define a problsm, and completement material in the AIPI course\",\n",
    "    'A2': \"No, the documents are not relevant enough to the question\",\n",
    "}\n",
    "\n",
    "GRADER_SYSTEM_PROMPT = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    All questions are in relation to the AIPI (Artificial Intelligence for Product Innovation) program. \\n\n",
    "    Do not answer the question itself. Only grade the relevance of the Documents context to the question.\n",
    "\n",
    "    DOCUMENTS: {context}\n",
    "    \"\"\"\n",
    "\n",
    "BINARY_INCLUSION_PROMPT = \"\\nAre the provided documents somewhat relevant to the question, answer YES OR NO\"\n",
    "\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": GRADER_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": FEW_SHOT_PROMPT['Q1']},\n",
    "    {\"role\": \"assistant\", \"content\": FEW_SHOT_PROMPT['A1']},\n",
    "    {\"role\": \"user\", \"content\": FEW_SHOT_PROMPT['Q2']},\n",
    "    {\"role\": \"assistant\", \"content\": FEW_SHOT_PROMPT['A2']},\n",
    "    {\"role\": \"system\", \"content\": GRADER_SYSTEM_PROMPT.format(context=matches['matches'][0]['metadata']['text'])},\n",
    "    {\"role\": \"user\", \"content\": Question+BINARY_INCLUSION_PROMPT},\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, temperature=0.1, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "response = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "\n",
    "# Perform regex matching to determine either positive or negative for grader\n",
    "positive_patterns = [r\"\\b(?:yes)\\b\", r\"\\b(?:are relevant)\\b\"]\n",
    "negative_patterns = [r\"\\b(?:no)\\b\", r\"\\b(?:are not relevant)\\b\"]\n",
    "\n",
    "positive_regex = re.compile(\"|\".join(positive_patterns), flags=re.IGNORECASE)\n",
    "negative_regex = re.compile(\"|\".join(negative_patterns), flags=re.IGNORECASE)\n",
    "\n",
    "is_positive = bool(positive_regex.search(response))\n",
    "\n",
    "is_negative = bool(negative_regex.search(response))\n",
    "\n",
    "if is_negative:\n",
    "    boolean_result = False\n",
    "elif is_positive:\n",
    "    boolean_result = True\n",
    "else:\n",
    "    boolean_result = True\n",
    "print(boolean_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
