{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"WhereIsAI/UAE-Large-V1\")\n",
    "\n",
    "website_data_path = '/kaggle/input/raw-website-data'\n",
    "\n",
    "docs_list = []\n",
    "for filename in os.listdir(website_data_path):\n",
    "    file_path = os.path.join(website_data_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            docs_list.append(f.read())\n",
    "            \n",
    "# Initialize the text splitter with the specified chunk size and overlap\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer, chunk_size=1000, chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "# Since `split_documents` function is not directly shown in the provided context,\n",
    "# assuming a function that accepts a list of documents and splits each accordingly\n",
    "documents = [text_splitter.split_text(doc) for doc in docs_list]\n",
    "flattened_documents = [chunk for doc_chunks in documents for chunk in doc_chunks]\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.page_content = text\n",
    "        self.metadata = {}\n",
    "\n",
    "# Create Document objects for each document string\n",
    "documents_with_attributes = [Document(chunk) for chunk in flattened_documents]\n",
    "\n",
    "# Define huggingface embeddings\n",
    "model_name = \"WhereIsAI/UAE-Large-V1\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = 'aipi-chatbot'\n",
    "docsearch = PineconeVectorStore.from_documents(documents_with_attributes, hf, index_name=index_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
