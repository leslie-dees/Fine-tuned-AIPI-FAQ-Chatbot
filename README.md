# Fine-tuned-AIPI-FAQ-Chatbot

## Main Pillars
The three main pillars of or project - 
1. Pinecone
- High speed search and scalability
- Better integration
- Easy Hosting
2. Mistral - 7B
- High Benchmark results
- Efficient Architecture
- Relatively lightweight
3. cRAG
- CRAG based pipeline hooked with tavily search
- Directs questions with low match to web search
- Enhances bots ability to handle out-of-context questions.

## Data
Our three main sources:
1. AIPI FAQ Document
2. Duke AIPI Web domain
- https://ai.meng.duke.edu/
- All subdomains attached to this master domain
3. Syllabus Information for AIPI courses
- Excluding certain ones we did not have access to, i.e. AIPI 560

### Preprocessing
1. Data Selection and Pruning
2. Vectorization and Tokenization - WhereIsAI/UAE-Large-V1
3. Chunking strategy
- Data Integrity and Leakage Prevention

## Finetuning
### Data - Databricks Dolly 15K 
- 15,000 human-generated instruction corpus specifically designed for training conversational AI
- Can be used, modified, and extended for any purpose, including academic or commercial applications
- Split the data as 80-20 for training and test split.

### Model - Mistral-7B-v0.1
- Open-source foundation language model with 7.3B params, outperforming larger models like LLaMA-13B & even LLaMA-34B 
- Efficient architecture with Grouped Query Attention (GQA) and Sliding Window Attention (SWA)
- Adaptability and Customization: Can be fine-tuned for specific domains, industries, or use cases.

#### Configurations
1. ChatML Model
- Don't need Instruct Tags
- Enhanced Format Clarity
- Optimized for RAG

For faster and for using less computation, the following were used - 
2. Bits and Bytes Quantized
- Models are loaded in 4-bit precision to decrease memory usage. 
- Utilizes ‘torch.bfloat16’ for computing, balancing performance and precision.

3. LoRA
Applies to all linear layers, enhancing the model’s ability to adapt.

4. Flash Attention
- Reduced Memory Footprint
- Increased Computational Speed
- Scalability

5. Additional
- Optimized for 1 GPU - Training & Inference 
- Optimizer – Adamw_bnb_8bit

### Results
The total cost for all experiments were ~100$ and Final model cost is ~$7: 
1. Training
- AWS g5.16xlarge
- GPU: Nvidia A10 (24GB)
- All experiments: 24 hrs
- Final model: 103 mins

2. Evaluation
- 1000 random samples
- LLM as Judge –  the LLaMa2-7B model serves as the standard, comparing the outputs generated by the Mistral bot against established ground truth data.
- Accuracy – 82.7% 





















## Finetuning

The finetuning pipeline is available in [`./finetune`](./finetune/)